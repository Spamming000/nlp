Fine tuning- Hugging face
https://kedion.medium.com/fine-tuning-nlp-models-with-hugging-face-f92d55949b66

Datasets
https://archive.ics.uci.edu/datasets

Pipeline
https://www.kaggle.com/code/nilaychauhan/getting-started-with-nlp-pipelines

BERT Expansion 
D/W  BERT n GPT 
D/W  Transformers n LSTM n Encoder-decoder 
What is , why Attention mechanism 
What is Attention vector  
What is given as input to BERT model 
How Bert is fine tuned for specific task 
explain encoder, deccoder architecture 
What is the base model for BERT 
How LSTM works 
what will be the input for encoders 
How Attention mech. works

Difference between tagging and parsing
Dependency parsing tree
And how it tackles ambiguity
Hmm vs something I don't remember
